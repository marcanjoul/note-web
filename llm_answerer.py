import subprocess  # Used to run external commands and interact with system processes.
import warnings  # Used to manage and suppress warning messages.
from urllib3.exceptions import NotOpenSSLWarning  # Specific warning type to suppress (related to urllib3 SSL).
warnings.filterwarnings("ignore", category=NotOpenSSLWarning)  # Suppress NotOpenSSLWarning warnings globally.

import os  # Used for environment variable management.
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # Prevents parallelism warnings from tokenizer libraries.

def build_prompt(query, top_chunks, history=None):
    """
    Constructs a prompt for the LLM using the user's query, top relevant text chunks, and optional chat history.

    Args:
        query (str): The user's question.
        top_chunks (List[dict]): List of dictionaries containing relevant text chunks.
        history (List[dict], optional): List of previous Q&A pairs for conversational context.

    Returns:
        str: The formatted prompt string to send to the LLM.
    """
    combined_chunks = ""  # Will hold concatenated context from top_chunks.
    for chunk in top_chunks:
        combined_chunks += chunk["text"] + "\n"  # Add each chunk's text, separated by newlines.

    history_prompt = ""  # Will hold formatted previous Q&A pairs if provided.
    if history:
        for pair in history:
            # Format each previous question and answer for conversational context.
            history_prompt += f"Q: {pair['question']}\nA: {pair['answer']}\n"

    # Compose the final prompt, including instructions, context, and the user's question.
    prompt = (
        "You're a helpful, friendly assistant. Use a casual, conversational tone and avoid sounding robotic. "
        "Answer questions based on the provided document(s) and use any previous conversation for context. "
        "Don't ask for greetings or introductionsâ€”just continue the chat naturally.\n"
        f"{'Conversation history:' + history_prompt if history_prompt else ''}"
        f"Context:\n{combined_chunks}\n"
        f"Question: {query}"
    )
    return prompt

def answer_with_llama(query, top_chunks, history=None):
    """
    Sends a prompt to the local Llama3 model via Ollama and returns the model's answer.

    Args:
        query (str): The user's question.
        top_chunks (List[dict]): List of relevant text chunks for context.
        history (List[dict], optional): Previous Q&A pairs for conversational context.

    Returns:
        str: The answer generated by the Llama3 model.
    """
    prompt = build_prompt(query, top_chunks, history)  # Build the prompt using context and history.

    # Run the Ollama command-line tool to interact with the Llama3 model.
    # The prompt is sent via stdin, and the output is captured.
    result = subprocess.run(
        ["ollama", "run", "llama3:instruct"],  # Command to run the Llama3 model in instruct mode.
        input=prompt,  # The prompt string to send to the model.
        text=True,  # Treat input and output as text (not bytes).
        capture_output=True  # Capture stdout and stderr for processing.
    )

    # Return the model's answer, stripped of leading/trailing whitespace.
    return result.stdout.strip()
